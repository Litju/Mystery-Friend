{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bab91187",
   "metadata": {},
   "source": [
    "# Mystery Friend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407f96fa",
   "metadata": {},
   "source": [
    "You've received an anonymous postcard from a friend who you haven't seen in years. Your friend did not leave a name, but the card is definitely addressed to you. So far, you've narrowed your search down to three friends, based on handwriting:\n",
    "- Emma Goldman\n",
    "- Matthew Henson\n",
    "- TingFang Wu\n",
    "\n",
    "But which one sent you the card?\n",
    "\n",
    "Just like you can classify a message as spam or not spam with a spam filter, you can classify writing as related to one friend or another by building a kind of friend writing classifier. You have past writing from all three friends stored up in the variable `friends_docs`, which means you can use scikit-learn's bag-of-words and Naive Bayes classifier to determine who the mystery friend is!\n",
    "\n",
    "Ready?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6790002",
   "metadata": {},
   "source": [
    "## Feature Vectors Are in the Bag with Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0a2f94",
   "metadata": {},
   "source": [
    "1. In the code block below, import `CountVectorizer` from `sklearn.feature_extraction.text`. Below it, import `MultinomialNB` from `sklearn.naive_bayes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90fe333e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn modules here:\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722292bb",
   "metadata": {},
   "source": [
    "2. Define `bow_vectorizer` as an implementation of `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ca48bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bow_vectorizer:\n",
    "bow_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6eb23c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: import_ipynb in c:\\users\\usuario\\anaconda3\\lib\\site-packages (0.1.4)\n",
      "Requirement already satisfied: IPython in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from import_ipynb) (8.15.0)\n",
      "Requirement already satisfied: nbformat in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from import_ipynb) (5.9.2)\n",
      "Requirement already satisfied: backcall in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from IPython->import_ipynb) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from IPython->import_ipynb) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from IPython->import_ipynb) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from IPython->import_ipynb) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from IPython->import_ipynb) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from IPython->import_ipynb) (3.0.36)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from IPython->import_ipynb) (2.15.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from IPython->import_ipynb) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from IPython->import_ipynb) (5.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from IPython->import_ipynb) (0.4.6)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from nbformat->import_ipynb) (2.16.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from nbformat->import_ipynb) (4.17.3)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from nbformat->import_ipynb) (5.3.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from jedi>=0.16->IPython->import_ipynb) (0.8.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat->import_ipynb) (22.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from jsonschema>=2.6->nbformat->import_ipynb) (0.18.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->IPython->import_ipynb) (0.2.5)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from jupyter-core->nbformat->import_ipynb) (3.10.0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from jupyter-core->nbformat->import_ipynb) (305.1)\n",
      "Requirement already satisfied: executing in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from stack-data->IPython->import_ipynb) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from stack-data->IPython->import_ipynb) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from stack-data->IPython->import_ipynb) (0.2.2)\n",
      "Requirement already satisfied: six in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from asttokens->stack-data->IPython->import_ipynb) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#the following code is a lightweight way to install new packages. You will need the `import_ipynb` package for this to \n",
    "%pip install import_ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ccd0d5",
   "metadata": {},
   "source": [
    "3. Use your newly minted `bow_vectorizer` to both `fit` (train) and `transform` (vectorize) all your friends' writing (stored in the variable `friends_docs`). Save the resulting vector object as `friends_vectors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9bc8de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from goldman_emma_raw.ipynb\n",
      "importing Jupyter notebook from henson_matthew_raw.ipynb\n",
      "importing Jupyter notebook from wu_tingfang_raw.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "\n",
    "from goldman_emma_raw import goldman_docs\n",
    "from henson_matthew_raw import henson_docs\n",
    "from wu_tingfang_raw import wu_docs\n",
    "\n",
    "friends_docs = goldman_docs + henson_docs + wu_docs\n",
    "\n",
    "# Define friends_vectors:\n",
    "friends_vectors = bow_vectorizer.fit_transform(friends_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde0b692",
   "metadata": {},
   "source": [
    "4. Create a new variable `mystery_vector`. Assign to it the vectorized form of `[mystery_postcard]` using the vectorizer's `.transform()` method.\n",
    "\n",
    "   (`mystery_postcard` is a string, while the vectorizer expects a list as an argument.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75520178",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystery_postcard = \"\"\"\n",
    "My friend,\n",
    "From the 10th of July to the 13th, a fierce storm raged, clouds of\n",
    "freeing spray broke over the ship, incasing her in a coat of icy mail,\n",
    "and the tempest forced all of the ice out of the lower end of the\n",
    "channel and beyond as far as the eye could see, but the _Roosevelt_\n",
    "still remained surrounded by ice.\n",
    "Hope to see you soon.\n",
    "\"\"\"\n",
    "\n",
    "# Define mystery_vector:\n",
    "mystery_vector = bow_vectorizer.transform([mystery_postcard])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6452a6b2",
   "metadata": {},
   "source": [
    "## This Mystery Friend Gets Classified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d60d795",
   "metadata": {},
   "source": [
    "5. You've vectorized and prepared all the documents. Let's take a look at your friends' writing samples to get a sense of how they write.\n",
    "\n",
    "   Print out one document of each friend's writing - try any one between `0` and `140`. (Your friends' documents are stored in `goldman_docs`, `henson_docs`, and `wu_docs`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6ce266c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " What he gives to the world is only gray and hideous\n",
      "things, reflecting a dull and hideous existence,--too weak to live,\n",
      "too cowardly to die\n",
      "Miss Marie Ahnighito Peary, aged about ten months, who\n",
      "first saw the light of day at Anniversary Lodge on the 12th of the\n",
      "previous September, was taken by her mother to her kinfolks in the\n",
      "South\n",
      " Let us, for instance, compare England with the United\n",
      "States\n"
     ]
    }
   ],
   "source": [
    "# Print out a document from each friend:\n",
    "print(goldman_docs[49])\n",
    "print(henson_docs[49])\n",
    "print(wu_docs[49])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5f608c",
   "metadata": {},
   "source": [
    "6. Have an inkling about which friend wrote the mystery card? We can use a classifier to confirm those suspicions...\n",
    "\n",
    "   Implement a Naive Bayes classifier using `MultinomialNB`. Save the result to `friends_classifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "571f4397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define friends_classifier:\n",
    "friends_classifier = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9dc9ca",
   "metadata": {},
   "source": [
    "7. Train `friends_classifier` on `friends_vectors` and `friends_labels` using the classifier's `.fit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53472f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "friends_labels = [\"Emma\"] * 154 + [\"Matthew\"] * 141 + [\"Tingfang\"] * 166\n",
    "\n",
    "# Train the classifier:\n",
    "friends_classifier.fit(friends_vectors, friends_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d6feaf",
   "metadata": {},
   "source": [
    "8. Change `predictions` value from `[\"None Yet\"]` to the classifier's prediction about which friend wrote the postcard. You can do this by calling the classifier's `predict()` method on the `mystery_vector`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4896da71",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = friends_classifier.predict(mystery_vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed18991f",
   "metadata": {},
   "source": [
    "## Mystery Revealed!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05086a66",
   "metadata": {},
   "source": [
    "9. Uncomment the final print statement and run the code block below to see who your mystery friend was all along!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9eaf82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The postcard was from Matthew!\n"
     ]
    }
   ],
   "source": [
    "mystery_friend = predictions[0] if predictions[0] else \"someone else\"\n",
    "\n",
    "# Uncomment the print statement:\n",
    "print(\"The postcard was from {}!\".format(mystery_friend))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d1f0a4",
   "metadata": {},
   "source": [
    "10. But does it really work? Find some lines by Emma Goldman, Matthew Henson, and TingFang Wu on <a href=\"http://www.gutenberg.org\" target=\"_blank\">gutenberg.org</a> and save them to `mystery_postcard` to see how the classifier holds up!\n",
    "\n",
    "    Try using the `.predict_proba()` method instead of `.predict()` and print out `predictions` to see the estimated probabilities that the `mystery_postcard` was written by each person.\n",
    "   \n",
    "    What happens when you add in a recent email or text instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e56747b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from goldman_emma_raw.ipynb\n",
      "importing Jupyter notebook from henson_matthew_raw.ipynb\n",
      "importing Jupyter notebook from wu_tingfang_raw.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules from scikit-learn\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Also ensure to import any other required modules or data\n",
    "import import_ipynb\n",
    "from goldman_emma_raw import goldman_docs\n",
    "from henson_matthew_raw import henson_docs\n",
    "from wu_tingfang_raw import wu_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d568992c",
   "metadata": {},
   "source": [
    " Define and Initialize Vectorizer and Classifier\n",
    "After importing the necessary modules, define and initialize the CountVectorizer and MultinomialNB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d59aa311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vectorizer and classifier\n",
    "bow_vectorizer = CountVectorizer()\n",
    "friends_classifier = MultinomialNB()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36128b36",
   "metadata": {},
   "source": [
    "Prepare and Vectorize Data\n",
    "Combine and vectorize the documents from each friend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d9d8b17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine documents from each friend\n",
    "friends_docs = goldman_docs + henson_docs + wu_docs\n",
    "\n",
    "# Vectorize friend documents\n",
    "friends_vectors = bow_vectorizer.fit_transform(friends_docs)\n",
    "\n",
    "# Define labels\n",
    "friends_labels = [\"Emma\"] * len(goldman_docs) + [\"Matthew\"] * len(henson_docs) + [\"Tingfang\"] * len(wu_docs)\n",
    "\n",
    "# Train the classifier\n",
    "friends_classifier.fit(friends_vectors, friends_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2163457f",
   "metadata": {},
   "source": [
    "Test with Sample Lines\n",
    "Define sample lines and vectorize them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be5ed99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emma Sample Prediction: ['Tingfang']\n",
      "Matthew Sample Prediction: ['Matthew']\n",
      "TingFang Sample Prediction: ['Emma']\n",
      "Emma Sample Probabilities: [[0.06435879 0.15218932 0.7834519 ]]\n",
      "Matthew Sample Probabilities: [[1.84761455e-05 9.99945592e-01 3.59317853e-05]]\n",
      "TingFang Sample Probabilities: [[0.6297412  0.29042627 0.07983254]]\n"
     ]
    }
   ],
   "source": [
    "# Sample lines from each friend (replace these with actual sample lines)\n",
    "emma_sample = \"\"\"\n",
    "The usual winter fever of America is in this month of January, and...\n",
    "\"\"\"\n",
    "matthew_sample = \"\"\"\n",
    "My expedition was a long and arduous one, but we made significant progress...\n",
    "\"\"\"\n",
    "tingfang_sample = \"\"\"\n",
    "The landscape was beautiful and serene, the perfect backdrop for...\n",
    "\"\"\"\n",
    "\n",
    "# Vectorize these samples\n",
    "emma_vector = bow_vectorizer.transform([emma_sample])\n",
    "matthew_vector = bow_vectorizer.transform([matthew_sample])\n",
    "tingfang_vector = bow_vectorizer.transform([tingfang_sample])\n",
    "\n",
    "# Predict\n",
    "print(\"Emma Sample Prediction:\", friends_classifier.predict(emma_vector))\n",
    "print(\"Matthew Sample Prediction:\", friends_classifier.predict(matthew_vector))\n",
    "print(\"TingFang Sample Prediction:\", friends_classifier.predict(tingfang_vector))\n",
    "\n",
    "# Print probabilities\n",
    "print(\"Emma Sample Probabilities:\", friends_classifier.predict_proba(emma_vector))\n",
    "print(\"Matthew Sample Probabilities:\", friends_classifier.predict_proba(matthew_vector))\n",
    "print(\"TingFang Sample Probabilities:\", friends_classifier.predict_proba(tingfang_vector))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d1722d",
   "metadata": {},
   "source": [
    "Test with Recent Email or Text\n",
    "Vectorize and classify a recent text sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "879a2886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recent Text Prediction: Matthew\n"
     ]
    }
   ],
   "source": [
    "# Recent email or text\n",
    "recent_text = \"\"\"\n",
    "Hi there, I hope you're doing well. Just wanted to catch up and see...\n",
    "\"\"\"\n",
    "\n",
    "# Vectorize the recent text\n",
    "recent_vector = bow_vectorizer.transform([recent_text])\n",
    "\n",
    "# Predict\n",
    "recent_prediction = friends_classifier.predict(recent_vector)\n",
    "print(\"Recent Text Prediction:\", recent_prediction[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b58d58f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
